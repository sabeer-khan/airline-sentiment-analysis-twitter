{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "US Airlines Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fc5y_i3ok4a"
      },
      "source": [
        "# **US Airlines Tweets Sentiment Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idJPyQKZjhkh"
      },
      "source": [
        "### **1. Problem Statement**\n",
        "In this problem, a sentiment analysis is performed on US Airlines from twitter data\n",
        "available @ https://www.kaggle.com/crowdflower/twitter-airline-sentiment with Na√Øve Bayes classifier. The tasks invloved in this project are as follows:\n",
        "\n",
        "* Build a dictionary based on your training corpus. Calculate conditional probability of each token for each class (this is also called unigram probability). Then evaluate on test data and report accuracy.\n",
        "* Try to improve your algorithm. Some suggestions: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i. Remove STOP words from the vocabulary that appear vary frequently but not related to the attitude or opinion of the writer. <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii. Reduce the size of your vocabulary further by taking only top-k frequent word types that appear in the training dataset. Vary k and compare performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQGYdtbxjhkp"
      },
      "source": [
        "### **2. Importing Libraries & Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx1eViXgjhkr"
      },
      "source": [
        "#importing the required libraries & packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from datetime import datetime\n",
        "import pytz"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuW8gH3cjhk4",
        "outputId": "ce39eb97-3323-4d7a-c8aa-1f40aed1319d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "#loading the US Airline Sentiment data\n",
        "data_frame = pd.read_csv('Tweets.csv')\n",
        "data_frame.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BETtlRyW2ZBq",
        "outputId": "b34a1e63-4aa1-4d0b-fcad-ab5e77dfa187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_frame.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QJceCmtjhlE"
      },
      "source": [
        "### **3. Data Preprocessing**\n",
        "\n",
        "The US Airline Sentiment dataset has so many features. Among them, in our project we will be working with the 'text' & 'airline_segment' features. Here, the 'text' is considered as a feature (X) and the 'airline_segment' as a target label (y) for the classifier.\n",
        "\n",
        "The values in the target data are categorical and are 'neutral', 'positive' & 'negative'. For the easy computation, we are replacing the 'neutral', 'positive' & 'negative' with 0, 1 & 2 values respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnWfUtMyjhlG",
        "outputId": "a67314a0-4833-4a86-cc3e-02d4ec86f831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#repalcing the categorical values of 'airline_sentiment' to numeric values\n",
        "data_frame['airline_sentiment'].replace(('neutral', 'positive', 'negative'), (0, 1, 2), inplace=True)\n",
        "data_frame['airline_sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    9178\n",
              "0    3099\n",
              "1    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx5SySKo_kvo"
      },
      "source": [
        "#forming the feature & label variables\n",
        "data = data_frame['text'].values.tolist()\n",
        "labels = data_frame['airline_sentiment'].values.tolist()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdA8JQ8_jhlQ",
        "outputId": "7bcbc94c-c1e7-4415-be78-023916cac5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#First five samples text\n",
        "data[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@VirginAmerica What @dhepburn said.',\n",
              " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
              " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
              " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
              " \"@VirginAmerica and it's a really big bad thing about it\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqhUtE4f_hAj",
        "outputId": "6bb3dc50-53f4-4223-a5c2-62e9bf507775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#first 5 samples label\n",
        "labels[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7U6hNbVjhlZ"
      },
      "source": [
        "### **4. Splitting the data for Classification**\n",
        "\n",
        "The data splitting is done in 80-20 split using trian_test_split method of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pduz91hcjhlb",
        "outputId": "08d71039-b35c-4f7a-9aca-82cddf29c673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#splitting the data into 80 and 20 split\n",
        "train_X, test_X, y_train, y_test = train_test_split(data, labels, test_size=0.2, \n",
        "                                                    random_state=42, shuffle=True)\n",
        "\n",
        "print(f'Number of training examples: {len(train_X)}')\n",
        "print(f'Number of testing examples: {len(test_X)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 11712\n",
            "Number of testing examples: 2928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_20Lfoqjhll"
      },
      "source": [
        "### **5. Text Preprocessing**\n",
        "\n",
        "A process of transforming text into something an algorithm can digest is text processing. This includes:\n",
        "* &nbsp;tokenizing the data\n",
        "* &nbsp;removing the punctuation\n",
        "* &nbsp;removing the stopwords\n",
        "* &nbsp;stemming \n",
        "* &nbsp;lemmatization\n",
        "\n",
        "As of now, we are only going to tokenize the data and work with it without removing the punctuation or stop words and apply any other text processing methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGX16pKBjhlm"
      },
      "source": [
        "# Here is a default pattern for tokenization\n",
        "default_pattern =  r\"\"\"(?x)                  \n",
        "                        (?:[A-Z]\\.)+          \n",
        "                        |\\$?\\d+(?:\\.\\d+)?%?    \n",
        "                        |\\w+(?:[-']\\w+)*      \n",
        "                        |\\.\\.\\.               \n",
        "                        |(?:[.,;\"'?():-_`])    \n",
        "                    \"\"\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkncRcuijhlx"
      },
      "source": [
        "#funtion for tokenizing the data\n",
        "\"\"\" Tokenize sentence with specific pattern\n",
        "Arguments: text {str} -- sentence to be tokenized, such as \"I love NLP\"\n",
        "Keyword Arguments: pattern {str} -- reg-expression pattern for tokenizer (default: {default_pattern})\n",
        "Returns: list -- list of tokenized words, such as ['I', 'love', 'nlp'] \"\"\"\n",
        "def tokenize(text, pattern = default_pattern):\n",
        "\n",
        "  text = text.lower()\n",
        "  return regexp_tokenize(text, pattern)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCTB0sWwjhl7"
      },
      "source": [
        "# Tokenize training text into tokens\n",
        "tokenized_text = []\n",
        "for i in range(0, len(train_X)):\n",
        "    tokenized_text.append(tokenize(train_X[i]))\n",
        "\n",
        "X_train = tokenized_text\n",
        "\n",
        "# Tokenize testing text into tokens\n",
        "tokenized_text = []\n",
        "for i in range(0, len(test_X)):\n",
        "    tokenized_text.append(tokenize(test_X[i]))\n",
        "\n",
        "X_test = tokenized_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7yG2O98jhmB",
        "outputId": "e310a4e3-63f2-4862-801a-854dec618507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#tokenized train & test data\n",
        "print(X_train[0], X_train[1])\n",
        "print(X_test[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'united', 'you', 'are', 'offering', 'us', '8', 'rooms', 'for', '32', 'people', 'fail'] ['@', 'jetblue', 'jfk', 'nyc', 'staff', 'is', 'amazing', '.', 'the', 'lax', 'jetblue', '...', 'sending', 'an', 'email', 'with', 'details', 'but', 'it', 'was', 'a', 'disappointing', 'experience', '@', 'jetbluecheeps']\n",
            "['@', 'southwestair', \"you're\", 'my', 'early', 'frontrunner', 'for', 'best', 'airline', 'oscars2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDhU3uA1VWiu"
      },
      "source": [
        "### **6. Building Dictionary**\n",
        "\n",
        "Building dictionary of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFyEv5ERPuB3"
      },
      "source": [
        "#building dictionary\n",
        "def createDictionary(data):\n",
        "  \"\"\" Function: To create a dictionary of tokens from the data\n",
        "  Arguments: data in the type - list\n",
        "  Returns: Sorted dictionary of the tokens and their count in the data \"\"\"\n",
        "\n",
        "  dictionary = dict()\n",
        "  for sample in  data:\n",
        "    for token in sample:\n",
        "      dictionary[token] = dictionary.get(token, 0) + 1\n",
        "  #sorting the dictionary based on the values\n",
        "  sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "  return dict(sorted_dict)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgbUxDeAU4oR",
        "outputId": "4f735112-b02b-41fa-9a96-357c8eb87f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "bog = createDictionary(X_train)\n",
        "#top 10 items in the dictionary\n",
        "print(\"Top 10 tokens in the training dictionary:\\n\")\n",
        "list(bog.items())[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 tokens in the training dictionary:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('@', 13290),\n",
              " ('.', 12534),\n",
              " ('to', 6858),\n",
              " ('the', 4856),\n",
              " ('i', 4385),\n",
              " ('?', 3729),\n",
              " ('a', 3619),\n",
              " (',', 3354),\n",
              " ('united', 3338),\n",
              " ('you', 3284)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0wL3PCpjhmH"
      },
      "source": [
        "### **7. Building the Navie Bayes Classifier**\n",
        "\n",
        "Now, we define text classifier class called NBClassifier, which comprises of three functions:\n",
        "* createDictionary()\n",
        "* fit()\n",
        "* predict()\n",
        "* score()\n",
        "\n",
        "**createDictionary():** This function takes in the tokenized text data and gives out the dictionary or the bag of words of the data.\n",
        "\n",
        "**fit():** This function has all the word counts required to calculate the Navie Bayes Classifier probabilities and then fits the classifier on our training data.\n",
        "\n",
        "**predict():** The test data is inputed to this function which determines the sentiment label based of each tweet by using the word counts computed during the training process (from fit function). In this step, Laplace smoothing is applied while computing Na√Øve Bayes probabilities for the test data.\n",
        "\n",
        "**score():** Determine how many tweets are classified correctly and measures the performance of the model in terms of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "auz1uD0IjhmI"
      },
      "source": [
        "#Navie Bayes Classifier \n",
        "class NBClassifier:\n",
        "\n",
        "    def __init__(self, X_train, y_train, size):\n",
        "      tz_NY = pytz.timezone('America/New_York') \n",
        "      print(\"Model Start Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.size = size\n",
        "\n",
        "    def createDictionary(self):\n",
        "      \"\"\" Function: To create a dictionary of tokens from the data\n",
        "      Arguments: data in the type - list\n",
        "      Returns: Sorted dictionary of the tokens and their count in the data \"\"\"\n",
        "      dictionary = dict()\n",
        "      for sample in  X_train:\n",
        "        for token in sample:\n",
        "          dictionary[token] = dictionary.get(token, 0) + 1\n",
        "      #sorting the dictionary based on the values\n",
        "      sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "      return dict(sorted_dict)\n",
        "    \n",
        "    def fit(self):\n",
        "      \"\"\" Function: To compute the count of words in training data dictionary\n",
        "        Arguments: Trianing data & Size of dictionary\n",
        "        Returns: dictionary of tokens with their class wise probabilities \"\"\"\n",
        "      \n",
        "      X_train_dict = self.createDictionary()\n",
        "      if self.size == 'full':\n",
        "        self.words_list = list(X_train_dict.keys())\n",
        "        self.words_count = dict.fromkeys(self.words_list, None)\n",
        "      else:\n",
        "        self.words_list = list(X_train_dict.keys())[:int(self.size)]\n",
        "        self.words_count = dict.fromkeys(self.words_list, None)\n",
        "            \n",
        "      #DataFrame of training data\n",
        "      train = pd.DataFrame(columns = ['X_train', 'y_train'])\n",
        "      train['X_train'] = X_train\n",
        "      train['y_train'] = y_train\n",
        "\n",
        "      train_0 = train.copy()[train['y_train'] == 0]\n",
        "      train_1 = train.copy()[train['y_train'] == 1]\n",
        "      train_2 = train.copy()[train['y_train'] == 2]\n",
        "\n",
        "      #computing the prior of each class\n",
        "      Pr0 = train_0.shape[0]/train.shape[0]\n",
        "      Pr1 = train_1.shape[0]/train.shape[0]\n",
        "      Pr2 = train_2.shape[0]/train.shape[0]\n",
        "      \n",
        "      self.Prior = np.array([Pr0, Pr1, Pr2])\n",
        "        \n",
        "      #converting list of lists into a list\n",
        "      def flatList(listOfList):\n",
        "        flatten = []\n",
        "        for elem in listOfList:\n",
        "          flatten.extend(elem)\n",
        "        return flatten\n",
        "  \n",
        "      #Creating the data list for each class - tokens of each class\n",
        "      X_train_0 = flatList(train[train['y_train'] == 0]['X_train'].tolist())\n",
        "      X_train_1 = flatList(train[train['y_train'] == 1]['X_train'].tolist())\n",
        "      X_train_2 = flatList(train[train['y_train'] == 2]['X_train'].tolist())\n",
        "    \n",
        "      self.X_train_len = np.array([len(X_train_0), len(X_train_1), len(X_train_2)])\n",
        "\n",
        "      for token in self.words_list:\n",
        "        #list to store three word counts of a token\n",
        "        res = []\n",
        "\n",
        "        #inserting count of token in class 0: Neutral\n",
        "        res.insert(0, X_train_0.count(token))\n",
        "\n",
        "        #inserting count of token in class 1: Positive\n",
        "        res.insert(1, X_train_1.count(token))\n",
        "\n",
        "          #inserting count of token in class 2: Negative\n",
        "        res.insert(2, X_train_2.count(token))\n",
        "\n",
        "        #assigning the count list to its token in the dictionary \n",
        "        self.words_count[token] = res\n",
        "      return self\n",
        "\n",
        "    def predict(self, X_test):\n",
        "      \"\"\" Function: Predicts the label of the data\n",
        "        Arguments: self and the test data\n",
        "        Returns: List of predicted labels for the test data \"\"\"     \n",
        "      pred = []\n",
        "      for sample in X_test:\n",
        "        mul = np.array([1,1,1])\n",
        "        for tokens in sample:\n",
        "          vocab_count = len(self.words_list)\n",
        "          if tokens in self.words_list:\n",
        "            prob = ((np.array(self.words_count[tokens])+1) / (self.X_train_len + vocab_count))\n",
        "          #except:\n",
        "            #prob = ((np.array([0,0,0])+1) / (self.X_train_len + vocab_count))\n",
        "          mul = mul * prob\n",
        "        val = mul * self.Prior\n",
        "        pred.append(np.argmax(val))\n",
        "      tz_NY = pytz.timezone('America/New_York') \n",
        "      print(\"Model End Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n",
        "      return pred\n",
        "    \n",
        "    def score(self, pred, labels):\n",
        "      \"\"\" Function: To compute the perfoemance of the model\n",
        "        Arguments: self, predicted labels and actual labels of the test data\n",
        "        Returns: Number of lables correctly predicted and the accuracy of the model \"\"\"\n",
        "      correct = (np.array(pred) == np.array(labels)).sum()\n",
        "      accuracy = correct/len(pred)\n",
        "      return correct, accuracy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM2bWIZ3jhmM"
      },
      "source": [
        "### **8. Navie Bayes Classifier Training and Evaluation**\n",
        "\n",
        "The Navie Bayes Classifier, NBClassifier takes three arguments:\n",
        "* X_train: Features of training dataset\n",
        "* y_train: Labels of training dataset\n",
        "* size: Size of vacabulary to be used in the model\n",
        "\n",
        "All three arguments are needed for the model to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYOgoiHUchMA"
      },
      "source": [
        "# Creating holders to store the model performance results\n",
        "attributes = []\n",
        "corr = []\n",
        "acc = []\n",
        "\n",
        "#function to call for storing the results\n",
        "def storeResults(attr, cor,ac):\n",
        "  attributes.append(attr)\n",
        "  corr.append(round(cor, 3))\n",
        "  acc.append(round(ac, 3))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0TqGF2sjhmN",
        "outputId": "413adf02-f940-45ee-b784-b817abd4ebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier     \n",
        "nb = NBClassifier(X_train, y_train, 'full')  \n",
        "nb.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:46:33\n",
            "Model End Time: 19:47:38\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLDjMGp0jhmR",
        "outputId": "cfc65d59-eb76-4014-cb26-1b1f8f538aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor1, acc1 = nb.score(y_pred, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor1)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor1, len(y_pred), acc1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2291\n",
            "Accuracy of the model: 2291 / 2928 = 0.7824 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtmKoVJnc5fB"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Unprocessed Data', cor1, acc1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTIhZD9QjhmV"
      },
      "source": [
        "The Navie Bayers Classifier that we trainined on the data predicts 78.24% of samples correctly. Now, to improve this number few more text processing methods are appilied on the training data and then the classifier is trained on this  modified data to predict the sentiment of the test samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVB4KAVpjhmW"
      },
      "source": [
        "### **9. Trying to improve the NBClassifier**\n",
        "\n",
        "To improve the performance of the NBClassifier, \n",
        "* apply other text processing methods\n",
        "* reduce the size of dictonary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBLcYlOlqI7v"
      },
      "source": [
        "#### **9.1. Further Processing Text Data**\n",
        "\n",
        "In this step, we are going to apply two text processing methods on the previously tokenized data:\n",
        "* remove the punctuation \n",
        "* remove stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka2j2vhDF1-6"
      },
      "source": [
        "##### **9.1.1. Remove Puntuation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIOaYPc_jhmW",
        "outputId": "d3cf5db9-9054-4ebb-9ed9-e269d93597af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#string of punctiations\n",
        "string.punctuation"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "45mxN-D2jhma"
      },
      "source": [
        "#Removing the punctuation\n",
        "'''Function: Removes the punctuation from the tokens\n",
        "   Arguments: list of text data samples\n",
        "   Returns: list of tokens of each sample without punctuation '''\n",
        "def removePunctuation(data):\n",
        "    update = []\n",
        "    for sample in data:\n",
        "        #removing punctuation from the tokens\n",
        "        re_punct = [''.join(char for char in word if char not in string.punctuation) for word in sample]\n",
        "        #removes the empty strings\n",
        "        re_punct = [word for word in re_punct if word]\n",
        "       \n",
        "        update.append(re_punct)\n",
        "    return update"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB3wla2tFLWv",
        "outputId": "db40520a-8ebd-422f-e60d-456049712b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Removing punctuation from training data text tokens  \n",
        "X_train_P = removePunctuation(X_train)\n",
        "\n",
        "#Removing punctuation from testing data text tokens\n",
        "X_test_P = removePunctuation(X_test)\n",
        "\n",
        "#train & test data after removing punctuation\n",
        "print(X_train_P[0])\n",
        "print(X_test_P[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['united', 'you', 'are', 'offering', 'us', '8', 'rooms', 'for', '32', 'people', 'fail']\n",
            "['southwestair', 'youre', 'my', 'early', 'frontrunner', 'for', 'best', 'airline', 'oscars2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLWE_rUUjhmk",
        "outputId": "15515a81-6955-4a0e-d312-2138783ebca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier     \n",
        "nb_punct = NBClassifier(X_train_P, y_train, 'full')\n",
        "nb_punct.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_P = nb_punct.predict(X_test_P)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_P))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:47:38\n",
            "Model End Time: 19:48:44\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PC7OsxL7Qyj",
        "outputId": "340cf75a-4bdf-43ab-b9dd-2a4335b8fbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor2, acc2 = nb_punct.score(y_pred_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor2)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor2, len(y_pred_P), acc2))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2285\n",
            "Accuracy of the model: 2285 / 2928 = 0.7804 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nGvM61Odq_C"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('No Punctuation Data', cor2, acc2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I86mv3RtHfmk"
      },
      "source": [
        "##### **9.1.2. Remove Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTW-U4iFHFQ"
      },
      "source": [
        "'''Function: Removes the stopwords from the tokens\n",
        "   Arguments: list of text data samples\n",
        "   Returns: list of tokens of each sample without punctuation '''\n",
        "def removeStopWords(data):\n",
        "    update = []\n",
        "    stopwords = ['the', 'at','i', 'of', 'us', 'have', 'a', 'you','ours', 'themselves', \n",
        "                 'that', 'this', 'be', 'is', 'for']\n",
        "    for sample in data:\n",
        "        #removing stopwords from tokenized data\n",
        "        re_stop = [word for word in sample if word not in stopwords]\n",
        "        \n",
        "        update.append(re_stop)\n",
        "    return update"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZZzVzpmjhmd",
        "outputId": "bab3a511-4130-48e2-acaa-959f5a3f9958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Removing stopwords from training data text tokens  \n",
        "X_train_S = removeStopWords(X_train)\n",
        "\n",
        "#Removing stopwords from testing data text tokens\n",
        "X_test_S = removeStopWords(X_test)\n",
        "\n",
        "#train & test data after removing stopwords\n",
        "print(X_train_S[0])\n",
        "print(X_test_S[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'united', 'are', 'offering', '8', 'rooms', '32', 'people', 'fail']\n",
            "['@', 'southwestair', \"you're\", 'my', 'early', 'frontrunner', 'best', 'airline', 'oscars2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYzud_H1FHQX",
        "outputId": "75689eff-303f-45ba-adf1-dd6331ed38d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier     \n",
        "nb_stop = NBClassifier(X_train_S, y_train, 'full')\n",
        "nb_stop.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_S = nb_stop.predict(X_test_S)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_S))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:48:44\n",
            "Model End Time: 19:49:47\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avvjr8QGjhmn",
        "outputId": "823b6a94-5adf-45aa-a476-7e06c16ffca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor3, acc3 = nb_stop.score(y_pred_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor3)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor3, len(y_pred_S), acc3))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2300\n",
            "Accuracy of the model: 2300 / 2928 = 0.7855 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deKrYJxEelgD"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Removed few Stopwords', cor3, acc3)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE1by_VTI27w"
      },
      "source": [
        "##### **9.1.3. Removing both Punctuation & Few Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0o0VhYRJFt_",
        "outputId": "5bca5ecf-d01b-4fa1-86bf-f3198d2d6cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Removing stopwords from training data text tokens  \n",
        "X_train_PS = removeStopWords(X_train_P)\n",
        "\n",
        "#Removing stopwords from testing data text tokens\n",
        "X_test_PS = removeStopWords(X_test_P)\n",
        "\n",
        "#train & test data after removing stopwords\n",
        "print(X_train_PS[0])\n",
        "print(X_test_PS[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['united', 'are', 'offering', '8', 'rooms', '32', 'people', 'fail']\n",
            "['southwestair', 'youre', 'my', 'early', 'frontrunner', 'best', 'airline', 'oscars2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4tzU0AlJFuK",
        "outputId": "d75b32c2-32a6-47da-c035-d4f9cd7477cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier     \n",
        "nb_PS = NBClassifier(X_train_PS, y_train, 'full')\n",
        "nb_PS.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_PS = nb_PS.predict(X_test_PS)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_PS))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:49:48\n",
            "Model End Time: 19:50:50\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHLQjGkJFuO",
        "outputId": "ba8981e7-7a3d-4c6d-a00a-404ee9877243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor4, acc4 = nb_PS.score(y_pred_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor4)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor4, len(y_pred_PS), acc4))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2283\n",
            "Accuracy of the model: 2283 / 2928 = 0.7797 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmX0dPEjucaA"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Removed both Punctuation & Few Stopwords', cor4, acc4)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkuNVUtRW0Oy"
      },
      "source": [
        "#### **9.2. Reducing the Dictionary Size**\n",
        "\n",
        "To improve the model performance, we reduce the size of training dictionary further by taking only top-k frequent word types that appear in it. Here, we vary the value of k and compare the model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07HMC5cgXhbY",
        "outputId": "fc042389-7f04-4a74-afd1-818c23fa29c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#total tokens in training dictionary\n",
        "print('Total tokens in the dictionary:', len(bog))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tokens in the dictionary: 13606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyM2TaO1Ugf9"
      },
      "source": [
        "##### **9.2.1. Considering Top 5k Tokens**\n",
        "\n",
        "**5k Tokens of Vocabulary - Unprocessed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5RLPdY2XfYX",
        "outputId": "010cb342-37bb-410f-ef17-1f034a8f1b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 5000 tokens \n",
        "nb_5k = NBClassifier(X_train, y_train, '5000')\n",
        "nb_5k.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k = nb_5k.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:50:50\n",
            "Model End Time: 19:51:15\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-D-QQcOXfYu",
        "outputId": "b3959bf7-5a07-44f3-b84a-d077900a7acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor5, acc5 = nb_5k.score(y_pred_5k, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor5)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor5, len(y_pred), acc5))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2332\n",
            "Accuracy of the model: 2332 / 2928 = 0.7964 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXLE0czuu9k"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Unprocessed Data', cor5, acc5)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq3t4Q8FboU4"
      },
      "source": [
        "**5k Tokens of Vocabulary - No Punctuation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAtviJUjboU6",
        "outputId": "411d8b7a-0d49-44da-f4a5-d2f7efe6f1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 5000 tokens \n",
        "nb_5k_P = NBClassifier(X_train_P, y_train, '5000')\n",
        "nb_5k_P.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_P = nb_5k_P.predict(X_test_P)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_P))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:51:15\n",
            "Model End Time: 19:51:40\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaaWsEiGboVN",
        "outputId": "b94a93cf-6275-4360-f693-e784986f9f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor6, acc6 = nb_5k.score(y_pred_5k_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor6)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor6, len(y_pred), acc6))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2309\n",
            "Accuracy of the model: 2309 / 2928 = 0.7886 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTvnor0fj9s8"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - No Punctuation Data', cor6, acc6)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXHL1dEOb4X7"
      },
      "source": [
        "**5k Tokens of Vocabulary - Removed few Stopwords**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U51l64Ob4X8",
        "outputId": "6a3c90a2-1ce1-45da-a1ab-eaf3391d0df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 5000 tokens \n",
        "nb_5k_S = NBClassifier(X_train_S, y_train, '5000')\n",
        "nb_5k_S.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_S = nb_5k_S.predict(X_test_S)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_S))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:51:40\n",
            "Model End Time: 19:52:05\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wKG4XjHb4YA",
        "outputId": "8898fb05-4f64-400c-b95a-3e93e174703e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor7, acc7 = nb_5k_S.score(y_pred_5k_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor7)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor7, len(y_pred), acc7))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2321\n",
            "Accuracy of the model: 2321 / 2928 = 0.7927 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGMbP2XykOut"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Removed few Stopwords', cor7, acc7)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlJ6bkSVpqTp"
      },
      "source": [
        "**5k Tokens of Vocabulary - Removed both Punctuation & Few Stopwords**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpRG7dI0pqTv",
        "outputId": "d457548c-ac26-41c5-8bc8-3cba968669df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 5000 tokens \n",
        "nb_5k_PS = NBClassifier(X_train_PS, y_train, '5000')\n",
        "nb_5k_PS.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_PS = nb_5k_PS.predict(X_test_PS)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_PS))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:52:05\n",
            "Model End Time: 19:52:28\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fmSSEzdpqT8",
        "outputId": "6ac7cc07-412a-4191-8699-2bd0c02c0c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor8, acc8 = nb_5k_PS.score(y_pred_5k_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor8)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor8, len(y_pred), acc8))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2296\n",
            "Accuracy of the model: 2296 / 2928 = 0.7842 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgXuV2wHpqUE"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor8, acc8)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cxUwo_mvLVZ"
      },
      "source": [
        "##### **9.2.2. Considering Top 10k Tokens**\n",
        "**10k Tokens of Vocabulary - Unprocessed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1tBdcQvLVe",
        "outputId": "1b9e0601-9737-4543-d2f0-56ede953bfe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 5000 tokens \n",
        "nb_10k = NBClassifier(X_train, y_train, '5000')\n",
        "nb_10k.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_10k = nb_10k.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:52:28\n",
            "Model End Time: 19:52:53\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqgZMo7MvLVt",
        "outputId": "99678648-0c11-434b-d88f-0e3023f12fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor9, acc9 = nb_10k.score(y_pred_10k, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor9)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor9, len(y_pred), acc9))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2332\n",
            "Accuracy of the model: 2332 / 2928 = 0.7964 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBALRXZsvLV0"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Unprocessed Data', cor9, acc9)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIRzN1PmvLV7"
      },
      "source": [
        "**10k Tokens of Vocabulary - No Punctuation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UCyGLm-vLV8",
        "outputId": "768defbb-b78c-4323-e42c-abc809048efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 10000 tokens \n",
        "nb_10k_P = NBClassifier(X_train_P, y_train, '10000')\n",
        "nb_10k_P.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_10k_P = nb_10k_P.predict(X_test_P)\n",
        "  \n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k_P))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:52:53\n",
            "Model End Time: 19:53:44\n",
            "NBClassifier Model miss any prediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnsMkZ9DvLWF",
        "outputId": "5d2b57ff-3085-43b2-eb5a-76bed43c9a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor10, acc10 = nb_10k_P.score(y_pred_10k_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor10)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor10, len(y_pred), acc10))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2287\n",
            "Accuracy of the model: 2287 / 2928 = 0.7811 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eIsPzvLWN"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - No Punctuation Data', cor10, acc10)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe1e-hhxvLWT"
      },
      "source": [
        "**10k Tokens of Vocabulary - Removed few Stopwords**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOt5et-PvLWV",
        "outputId": "061a596b-ce1a-483f-8c6d-5073c77112ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the classifier - 10000 tokens \n",
        "nb_10k_S = NBClassifier(X_train_S, y_train, '10000')\n",
        "nb_10k_S.fit()\n",
        "\n",
        "#Sredicting the labels for test samSles\n",
        "y_pred_10k_S = nb_10k_S.predict(X_test_S)\n",
        "  \n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any Srediction???\", len(X_test) != len(y_pred_10k_S))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:53:44\n",
            "Model End Time: 19:54:27\n",
            "NBClassifier Model miss any Srediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYEMhzVvLWa",
        "outputId": "eb43c4ce-a64d-4029-c2ce-342809edeeae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor11, acc11 = nb_10k_S.score(y_pred_10k_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor11)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor11, len(y_pred), acc11))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2321\n",
            "Accuracy of the model: 2321 / 2928 = 0.7927 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBngQ4tLvLWf"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Removed few Stopwords', cor11, acc11)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egUb3LyVvLWi"
      },
      "source": [
        "**10k Tokens of Vocabulary - Removed both Punctuation & Few Stopwords**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGR0Tu9FvLWi",
        "outputId": "8181cfe1-259f-4324-f859-5e44afabbfdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#training the claPSPSifier - 10000 tokenPS \n",
        "nb_10k_PS = NBClassifier(X_train_PS, y_train, '10000')\n",
        "nb_10k_PS.fit()\n",
        "\n",
        "#PSredicting the labelPS for tePSt PSamPSlePS\n",
        "y_pred_10k_PS = nb_10k_PS.predict(X_test_PS)\n",
        "  \n",
        "#Checking\n",
        "print(\"NBClaPSPSifier Model miSS any PSrediction???\", len(X_test) != len(y_pred_10k_PS))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Start Time: 19:54:27\n",
            "Model End Time: 19:55:14\n",
            "NBClaPSPSifier Model miSS any PSrediction??? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjHTZzELvLWl",
        "outputId": "290d3799-9251-477d-d327-330369732f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Performance of the classifier\n",
        "cor12, acc12 = nb_10k_PS.score(y_pred_10k_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor12)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor12, len(y_pred), acc12))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Correct Predictions: 2293\n",
            "Accuracy of the model: 2293 / 2928 = 0.7831 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oh7WgpvLWq"
      },
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor12, acc12)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgnjsl6mrRpI"
      },
      "source": [
        "### **10. Comparing the Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clKeFYmweE8E"
      },
      "source": [
        "#creating dataframe\n",
        "results = pd.DataFrame({ 'Data Modification': attributes,    \n",
        "    'Correct Predictions': corr,\n",
        "    'Model Accuracy': acc})"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZIZL7WneE2v",
        "outputId": "3a934094-81a7-487d-b4f9-807727f11417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "results.sort_values(by=['Model Accuracy', 'Correct Predictions'], ascending=False)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data Modification</th>\n",
              "      <th>Correct Predictions</th>\n",
              "      <th>Model Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5k Tokens of Voab - Unprocessed Data</td>\n",
              "      <td>2332</td>\n",
              "      <td>0.796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10k Tokens of Voab - Unprocessed Data</td>\n",
              "      <td>2332</td>\n",
              "      <td>0.796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5k Tokens of Voab - Removed few Stopwords</td>\n",
              "      <td>2321</td>\n",
              "      <td>0.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10k Tokens of Voab - Removed few Stopwords</td>\n",
              "      <td>2321</td>\n",
              "      <td>0.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5k Tokens of Voab - No Punctuation Data</td>\n",
              "      <td>2309</td>\n",
              "      <td>0.789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Removed few Stopwords</td>\n",
              "      <td>2300</td>\n",
              "      <td>0.786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5k Tokens of Voab - Removed both Punctuation &amp;...</td>\n",
              "      <td>2296</td>\n",
              "      <td>0.784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10k Tokens of Voab - Removed both Punctuation ...</td>\n",
              "      <td>2293</td>\n",
              "      <td>0.783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unprocessed Data</td>\n",
              "      <td>2291</td>\n",
              "      <td>0.782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10k Tokens of Voab - No Punctuation Data</td>\n",
              "      <td>2287</td>\n",
              "      <td>0.781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Punctuation Data</td>\n",
              "      <td>2285</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Removed both Punctuation &amp; Few Stopwords</td>\n",
              "      <td>2283</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    Data Modification  ...  Model Accuracy\n",
              "4                5k Tokens of Voab - Unprocessed Data  ...           0.796\n",
              "8               10k Tokens of Voab - Unprocessed Data  ...           0.796\n",
              "6           5k Tokens of Voab - Removed few Stopwords  ...           0.793\n",
              "10         10k Tokens of Voab - Removed few Stopwords  ...           0.793\n",
              "5             5k Tokens of Voab - No Punctuation Data  ...           0.789\n",
              "2                               Removed few Stopwords  ...           0.786\n",
              "7   5k Tokens of Voab - Removed both Punctuation &...  ...           0.784\n",
              "11  10k Tokens of Voab - Removed both Punctuation ...  ...           0.783\n",
              "0                                    Unprocessed Data  ...           0.782\n",
              "9            10k Tokens of Voab - No Punctuation Data  ...           0.781\n",
              "1                                 No Punctuation Data  ...           0.780\n",
              "3            Removed both Punctuation & Few Stopwords  ...           0.780\n",
              "\n",
              "[12 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAL7fbwWs-1P"
      },
      "source": [
        "**NOTE: Detailed description & analysis of each step are mentioned in the report.**"
      ]
    }
  ]
}